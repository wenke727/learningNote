# 机器学习算法

机器学习算法 = 模型表征 + 模型评估 + 优化算法

## SVM

模型表征： 线性分类模型
评估指标： 最大间隔

## 逻辑回归

## 决策树

一种自上而下，对数据样本进行树形分类的过程，由结点和有向边组成。
将决策树应用继承学习的思路可以得到随机森林、梯度提升决策树等模型

决策树的生成包括：

- 特征选择
- 树的构建
- 树的减枝

构建决策树常用算法：

- ID3 最大信息增益
  反应给定条件后不确定性减少的过程，特征取值越多意味着确定性更大，也就是条件熵更小，信息增益越大
- C4.5 最大信息增益比
  一定程度上对取值较多的特征进行惩罚，避免ID3出现过拟合的特征，提升决策树的泛化能力
- CART(Classification and regression tree)
  原理：最大基尼系数
  Gini描述的是数据的纯度

决策树的剪枝

- 预剪枝
  - 核心思想
    - 在树中节点进行扩展之前，先计算当前的划分是否可以带来模型的泛化能力
  - 方法：
    - 树达到了一定的深度
    - 当到达节点的样本数量小于某个阈值的时候，停止树的生长
    - 计算每次分裂对测试集的准确度的提升，当小于某个阈值的时候不再扩展
  - 优点
    - 思想直接、算法简单、效率高、适合大规模的问题
  - 缺点
    - 准确评估何时停止树的生长，需要经验判断
    - 预剪枝有一定的局限性，容易欠拟合
- 后剪枝
  - 核心思想
    - 让算法生成一棵完全生长的决策树，然后从底层往上计算是否需要剪枝
  - 方法
    - 错误率降低剪枝
    - 悲观剪枝
    - 代价复杂度剪枝
    - 最小误差剪枝
    - Critical Value Pruning(CVP)
  - 优缺点
    - 相对于预剪枝，后剪枝的方法通常可以得到泛化能力更强的决策树，但时间开销高

## 4 降维

- PCA 主成分分析
  - 中心思想
    最大化投影方差，也就是让数据在主轴上投影的方差最大
  - 优缺点
    不考虑数据的类别，只是把原始数据映射到一些方差比较大的方向上

- LDA 线性判断分析（一种有监督的降维算法）
  - 中心思想
    - 最大化类间距离、最小化类内距离

## 5 聚类

聚类算法往往是通过多次迭代来找到数据的最优分割点
EM算法， 最大期望算法, Expectation-Maxomization algorithm

- Kmeans
  - 核心思想
    - 通过迭代的方式寻找K个cluster的一种划分方案，使得聚类结果对应的代价最小
  - 步骤
    1. 初始化聚类中心
      根据给定的k值随机选取k个样本作为初始的聚类中心
    2. 根据聚类中心划分簇
      计算每个样本与各个聚类中心之间的距离，把每个样本分配给距离他最近的聚类中心
    3. 重新选择聚类中心
      将每个聚类中所有样本的平均值确定为新的聚类中心
    4. 停止移动
      重复第2步，直到聚类中心不再移动或达到最大迭代次数为止
  - 缺点
    - 受初值和离群点的影响，结果不稳定，肯能是局部最优
    - 无法很好地解决分布较大的情况
    - 不适合离散分类
  - 调优思路
    - 数据归一化和离群点处理
    - 合理选择K值，基于多次实验或者经验；损失函数拐点
    - 采用核函数
  - 改进模型
    - K-means++
      - 选取第n+1个聚类中心时，距离当前n个中心越远的点会有更高的概率被选为中心
    - ISODATA
      - 思想：但属于某个类别的样本数量偏少，删除；属于某个类别的样本数量太多，分列
      - 增加操作：分类操作；合并操作
      - 输入参数：预期的聚类中心K；Nmin；最大方差sigma；两个聚类中心之间所允许的最小距离

- 高斯混合模型（GMM， Gaussian Mixed Model）
  - 核心思想
    - 假设数据可以看作从多个高斯分布中生成出来的
    - 主要参数： 均值，方差，生成数据的概率/权重
  - 迭代过程：
    - E步骤：计算每个点由某个分模型生成的概率
    - M步骤：使用E步骤估算出的概率，来改进每个分模线的均值、方差和权重

- 自组织映射神经网络(Self-Organizing Map, SOM)

- DBSCAN(Density-based spatial clustering of applications with noise)
  - 核心思想
    - 是一种基于密度的空间聚类算法
    - Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density

- 评估
  - 常见数据簇
    - 以中心定义的数据簇
    - 以密度定义的数据簇
    - 以连通性定义的数据簇
    - 以概念定义的数据簇
  - 聚类评估
    - 估计聚类趋势
    - 判定数据簇数
    - 测定聚类质量
      通过考察数据簇的`分离情况`和`紧凑情况`来评估
      - 轮廓系数
      - 均方根平均偏差
      - R方

## 6 概率图模型

用观测节点表示观测到的数据，用隐含的节点表示潜在的知识，用边来描述知识和数据的相互关系，最后基于关系图获得一个概率分布

- 马尔可夫链
  - 满足无后效性的随机过程，在随机过程中，tn时刻的状态仅仅与前一状态有关
- 隐马尔可夫链
  - 含有未知参数的马尔可夫链进行建模的生成模型
  - 隐状态的转移概率
  - 隐状态到观测状态的输出概率

## 7 优化算法

损失函数定义了模型的评估特征

- 批量梯度下降法
- 随机梯度下降法
- 小批量梯度下降法

随机梯度下降法的加速

- 惯性保持：动量方法
- AdaGrad：环境感知
- Adam： 动量方法+环境感知

L1正则化与稀疏性

## 8 采样

不均匀样本集的重采样

最简单的处理不均匀样本集的方法就是随机采样

## 9 前馈神经网络

### 激活函数

- Sigmoid
- Tanh
- Relu
  - 优点
    - 只需要一个阈值即可得到激活值，计算简单
    - 非饱和性有效解决梯度消失问题，提供相对较宽的激活边界
    - 单侧抑制提供了神经网络的稀疏表达能力
  - 局限性
    - 训练过程中导致神经元死亡的问题
- Leaky ReLU
  - a的选择增加了问题难度，需要较强的人工先验知识或多次重复训练确定合适的参数值
- Parametric ReLU

### 神经网络训练技巧

- Dropout
  - 以一定的概率随机地“临时丢弃”一部分神经元
  - 在小批量级别的操作上，提供了一种轻量级的Bagging集成近似
- 批量归一化
  - 针对每一批数据，在网络的每一层输入之前增加归一化处理
  - 看作在每一层输入和上一层输出之间加入了一个新的计算层，对数据的分布进行额外的约束，从而提高模型的泛化能力

### 深度卷积神经网络（CNN）

每层的神经元只响应前一局部区域范围内的神经元

一个深度卷积神经网络模型通常又若干卷积层叠加若干全连接层组成，中间也包含各种非线性操作以及池化操作

卷积操作的本质特征：

- 稀疏交互
  - 每层的神经元只响应前一局部区域范围内的神经元
  - 物理含义
    - 先学习局部特征，再将局部特征组合起来形成起来更复杂和抽象的特征
- 参数共享
  - 同一个模型的不同模块中使用相同的参数
  - 卷积核的每一个元素作用于每一次局部输入的特定位置上
  - 物理含义
    - 使得卷积层具有平移等变性

池化

